{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import cv2\n",
    "import itertools\n",
    "import numpy as np\n",
    "from time import time\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from skimage import transform\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the mediapipe face detection class\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "# Setup the face detection function\n",
    "face_detection = mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.5)\n",
    "#initialize the mediapipe face mesh class\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "# Setup the face landmarks function for videos\n",
    "face_mesh_videos = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1,\n",
    "                                         min_detection_confidence=0.5,min_tracking_confidence=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_video(filename):\n",
    "    cap = cv2.VideoCapture(filename)                                             \n",
    "    while(cap.isOpened()):                                                       \n",
    "        ret, frame = cap.read() # BGR                                            \n",
    "        if ret:                                                                  \n",
    "            yield frame                                                          \n",
    "        else:                                                                    \n",
    "            break                                                                \n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def save2npz(filename, data=None):\n",
    "    if filename[-4:] != '.npz':\n",
    "        filename = filename + '.npz'                           \n",
    "    if not os.path.exists(os.path.dirname(filename)):                            \n",
    "        os.makedirs(os.path.dirname(filename))                                   \n",
    "    np.savez_compressed(filename, data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_face_points(video):\n",
    "    vid_capture = cv2.VideoCapture(video)\n",
    "    all_points = []\n",
    "    points_data_regex = re.compile(r'\\d\\.\\d+')\n",
    "    time1 = 0\n",
    "\n",
    "    if (vid_capture.isOpened() == False):\n",
    "        print(\"Error opening the video file\")\n",
    "    else:\n",
    "        while(vid_capture.isOpened()):\n",
    "            ret, frame = vid_capture.read()\n",
    "            if ret == True:\n",
    "                mesh_result = face_mesh_videos.process(frame)\n",
    "                oval_indexes = list(set(itertools.chain(*mp_face_mesh.FACEMESH_FACE_OVAL)))\n",
    "                lips_indexes = list(set(itertools.chain(*mp_face_mesh.FACEMESH_LIPS)))\n",
    "                points_per_frame = {}\n",
    "                if mesh_result.multi_face_landmarks:\n",
    "                    # Iterate over the found faces.\n",
    "                    temp_oval_list = [None] * len(oval_indexes)\n",
    "                    temp_lips_list = [None] * len(lips_indexes)\n",
    "                    for face_no, face_landmarks in enumerate(mesh_result.multi_face_landmarks):\n",
    "                        for count, each_oval_index in enumerate(oval_indexes):\n",
    "                            xyzpointsraw = face_landmarks.landmark[each_oval_index]\n",
    "                            points_list = points_data_regex.findall(str(xyzpointsraw))\n",
    "                            if len(points_list) < 1:\n",
    "                                pass\n",
    "                            else:\n",
    "                                xyclean = [float(points_list[0]), float(points_list[1])]\n",
    "                                temp_oval_list[count] = xyclean\n",
    "                        for count, each_lip_index in enumerate(lips_indexes):\n",
    "                            xyzpointsraw = face_landmarks.landmark[each_lip_index]\n",
    "                            points_list = points_data_regex.findall(str(xyzpointsraw))\n",
    "                            if len(points_list) < 1:\n",
    "                                pass\n",
    "                            else:\n",
    "                                xyclean = [float(points_list[0]), float(points_list[1])]\n",
    "                                temp_lips_list[count] = xyclean\n",
    "\n",
    "                    points_per_frame['oval_landmarks'] = np.array(temp_oval_list)\n",
    "                    points_per_frame['lips_landmarks'] = np.array(temp_lips_list)\n",
    "                    all_points.append(points_per_frame)\n",
    "                else:\n",
    "                    points_per_frame['oval_landmarks'] = np.array([None])\n",
    "                    points_per_frame['lips_landmarks'] = np.array([None])\n",
    "                    all_points.append(points_per_frame)\n",
    "            else:\n",
    "                break\n",
    "            time2 = time()\n",
    "\n",
    "            time1 = time2\n",
    "            \n",
    "            #k = cv2.waitKey(30) & 0xFF\n",
    "            k = cv2.waitKey(10)\n",
    "            if(k == 27):\n",
    "                break\n",
    "        vid_capture.release()\n",
    "    return np.array(all_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def extract_multi_internal(video, landmarks_path, which_folder):\n",
    "    file = os.path.basename(video)\n",
    "    word = file.split(\"_\")[0]\n",
    "    points_array = get_face_points(video)\n",
    "    new_savepath = os.path.join(landmarks_path, word, which_folder, file.rstrip('.mp4'))\n",
    "    save2npz(new_savepath, data=points_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def extract_face_points_and_save(lrw_direc, landmarks_path, which_folder):\n",
    "    videos = glob.glob(os.path.join(lrw_direc, '*', which_folder, '*.mp4'))\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = [executor.submit(extract_multi_internal, video, landmarks_path, which_folder) for video in videos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def multi_extract_save(lrw_direc, landmarks_path, which_folder):\n",
    "    videos = glob.glob(os.path.join(lrw_direc, '*', which_folder, '*.mp4'))\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        for video, output_array in tqdm(executor.map(get_face_points, videos)):\n",
    "            file = os.path.basename(video)\n",
    "            word = file.split(\"_\")[0]\n",
    "            new_savepath = os.path.join(landmarks_path, word, which_folder, file.rstrip('.mp4'))\n",
    "            save2npz(new_savepath, data=output_array)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def single_extract_face_points_and_save(lrw_direc, landmarks_path, which_folder):\n",
    "    videos = glob.glob(os.path.join(lrw_direc, '*', which_folder, '*.mp4'))\n",
    "    for video in tqdm(videos):\n",
    "        extract_multi_internal(video, landmarks_path, which_folder)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 100/25000 [00:16<1:09:01,  6.01it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [15]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m landmarks_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/media/taylorpap/1TBM2/DatasetML/Capstone/LANDMARKS\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m      3\u001B[0m which_folder \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m----> 4\u001B[0m \u001B[43msingle_extract_face_points_and_save\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlrw_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlandmarks_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwhich_folder\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [13]\u001B[0m, in \u001B[0;36msingle_extract_face_points_and_save\u001B[0;34m(lrw_direc, landmarks_path, which_folder)\u001B[0m\n\u001B[1;32m      2\u001B[0m videos \u001B[38;5;241m=\u001B[39m glob\u001B[38;5;241m.\u001B[39mglob(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(lrw_direc, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m'\u001B[39m, which_folder, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m*.mp4\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m video \u001B[38;5;129;01min\u001B[39;00m tqdm(videos):\n\u001B[0;32m----> 4\u001B[0m     \u001B[43mextract_multi_internal\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvideo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlandmarks_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwhich_folder\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [10]\u001B[0m, in \u001B[0;36mextract_multi_internal\u001B[0;34m(video, landmarks_path, which_folder)\u001B[0m\n\u001B[1;32m      2\u001B[0m file \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mbasename(video)\n\u001B[1;32m      3\u001B[0m word \u001B[38;5;241m=\u001B[39m file\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m----> 4\u001B[0m points_array \u001B[38;5;241m=\u001B[39m \u001B[43mget_face_points\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvideo\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m new_savepath \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(landmarks_path, word, which_folder, file\u001B[38;5;241m.\u001B[39mrstrip(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.mp4\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[1;32m      6\u001B[0m save2npz(new_savepath, data\u001B[38;5;241m=\u001B[39mpoints_array)\n",
      "Input \u001B[0;32mIn [9]\u001B[0m, in \u001B[0;36mget_face_points\u001B[0;34m(video)\u001B[0m\n\u001B[1;32m     11\u001B[0m ret, frame \u001B[38;5;241m=\u001B[39m vid_capture\u001B[38;5;241m.\u001B[39mread()\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ret \u001B[38;5;241m==\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m---> 13\u001B[0m     mesh_result \u001B[38;5;241m=\u001B[39m \u001B[43mface_mesh_videos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mframe\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m     oval_indexes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mset\u001B[39m(itertools\u001B[38;5;241m.\u001B[39mchain(\u001B[38;5;241m*\u001B[39mmp_face_mesh\u001B[38;5;241m.\u001B[39mFACEMESH_FACE_OVAL)))\n\u001B[1;32m     15\u001B[0m     lips_indexes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mset\u001B[39m(itertools\u001B[38;5;241m.\u001B[39mchain(\u001B[38;5;241m*\u001B[39mmp_face_mesh\u001B[38;5;241m.\u001B[39mFACEMESH_LIPS)))\n",
      "File \u001B[0;32m~/anaconda3/envs/projectenv/lib/python3.10/site-packages/mediapipe/python/solutions/face_mesh.py:124\u001B[0m, in \u001B[0;36mFaceMesh.process\u001B[0;34m(self, image)\u001B[0m\n\u001B[1;32m    109\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprocess\u001B[39m(\u001B[38;5;28mself\u001B[39m, image: np\u001B[38;5;241m.\u001B[39mndarray) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m NamedTuple:\n\u001B[1;32m    110\u001B[0m   \u001B[38;5;124;03m\"\"\"Processes an RGB image and returns the face landmarks on each detected face.\u001B[39;00m\n\u001B[1;32m    111\u001B[0m \n\u001B[1;32m    112\u001B[0m \u001B[38;5;124;03m  Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    121\u001B[0m \u001B[38;5;124;03m    face landmarks on each detected face.\u001B[39;00m\n\u001B[1;32m    122\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[0;32m--> 124\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mimage\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mimage\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/projectenv/lib/python3.10/site-packages/mediapipe/python/solution_base.py:334\u001B[0m, in \u001B[0;36mSolutionBase.process\u001B[0;34m(self, input_data)\u001B[0m\n\u001B[1;32m    328\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    329\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_graph\u001B[38;5;241m.\u001B[39madd_packet_to_input_stream(\n\u001B[1;32m    330\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream_name,\n\u001B[1;32m    331\u001B[0m         packet\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_packet(input_stream_type,\n\u001B[1;32m    332\u001B[0m                                  data)\u001B[38;5;241m.\u001B[39mat(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_simulated_timestamp))\n\u001B[0;32m--> 334\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_graph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait_until_idle\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    335\u001B[0m \u001B[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001B[39;00m\n\u001B[1;32m    336\u001B[0m \u001B[38;5;66;03m# output stream names.\u001B[39;00m\n\u001B[1;32m    337\u001B[0m solution_outputs \u001B[38;5;241m=\u001B[39m collections\u001B[38;5;241m.\u001B[39mnamedtuple(\n\u001B[1;32m    338\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSolutionOutputs\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_stream_type_info\u001B[38;5;241m.\u001B[39mkeys())\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "lrw_path = '/media/taylorpap/1TBM2/DatasetML/lipread_mp4'\n",
    "landmarks_path = '/media/taylorpap/1TBM2/DatasetML/Capstone/LANDMARKS'\n",
    "which_folder = 'test'\n",
    "single_extract_face_points_and_save(lrw_path, landmarks_path, which_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488766/488766 [00:05<00:00, 85978.05it/s] \n"
     ]
    }
   ],
   "source": [
    "lrw_path = '/media/taylorpap/1TBM2/DatasetML/lipread_mp4'\n",
    "landmarks_path = '/media/taylorpap/1TBM2/DatasetML/Capstone/LANDMARKS'\n",
    "which_folder = 'train'\n",
    "multi_extract_save(lrw_path, landmarks_path, which_folder)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 9942, 'most_recent_fitting_scores': array([1.3021501, 1.2289561, 1.3895243], dtype=float32), 'facial_landmarks': array([[ 67.37307 , 102.33998 ],\n",
      "        [ 67.9815  , 118.51357 ],\n",
      "        [ 69.84342 , 134.71419 ],\n",
      "        [ 72.836365, 150.84415 ],\n",
      "        [ 78.37621 , 166.20525 ],\n",
      "        [ 87.10389 , 180.23193 ],\n",
      "        [ 98.489395, 192.255   ],\n",
      "        [112.558205, 201.30927 ],\n",
      "        [129.11067 , 203.89828 ],\n",
      "        [145.84485 , 201.25172 ],\n",
      "        [160.18242 , 192.10963 ],\n",
      "        [171.65858 , 179.50352 ],\n",
      "        [180.0401  , 164.99963 ],\n",
      "        [185.3956  , 149.13167 ],\n",
      "        [188.69629 , 132.62656 ],\n",
      "        [190.53842 , 116.278114],\n",
      "        [191.26785 , 100.21831 ],\n",
      "        [ 81.380684,  84.86308 ],\n",
      "        [ 87.70085 ,  77.79525 ],\n",
      "        [ 97.19054 ,  74.330025],\n",
      "        [107.55041 ,  74.920815],\n",
      "        [116.74992 ,  78.82953 ],\n",
      "        [141.00378 ,  78.73148 ],\n",
      "        [150.43372 ,  74.73129 ],\n",
      "        [160.95847 ,  74.01238 ],\n",
      "        [170.9602  ,  77.6806  ],\n",
      "        [177.3361  ,  85.20602 ],\n",
      "        [128.71687 ,  93.45032 ],\n",
      "        [128.71605 , 103.98411 ],\n",
      "        [128.61615 , 114.18432 ],\n",
      "        [128.60896 , 124.881195],\n",
      "        [115.42211 , 132.9761  ],\n",
      "        [121.6657  , 135.67699 ],\n",
      "        [128.49988 , 137.38397 ],\n",
      "        [135.49721 , 135.8283  ],\n",
      "        [141.73135 , 133.44722 ],\n",
      "        [ 92.79299 ,  96.0925  ],\n",
      "        [ 99.01467 ,  92.230194],\n",
      "        [105.79491 ,  92.03212 ],\n",
      "        [112.33554 ,  96.246735],\n",
      "        [105.76007 ,  97.329605],\n",
      "        [ 99.05362 ,  97.32528 ],\n",
      "        [146.2678  ,  96.24984 ],\n",
      "        [152.71118 ,  92.38102 ],\n",
      "        [159.52089 ,  92.797844],\n",
      "        [165.7502  ,  96.634026],\n",
      "        [159.71469 ,  97.858345],\n",
      "        [152.75241 ,  97.53926 ],\n",
      "        [104.41703 , 156.29495 ],\n",
      "        [112.5759  , 150.92378 ],\n",
      "        [121.889015, 148.77892 ],\n",
      "        [128.03548 , 150.34854 ],\n",
      "        [134.86168 , 148.9919  ],\n",
      "        [144.49521 , 151.57822 ],\n",
      "        [153.00102 , 157.18736 ],\n",
      "        [144.86594 , 165.7748  ],\n",
      "        [135.40338 , 169.96729 ],\n",
      "        [128.03609 , 170.55072 ],\n",
      "        [121.14372 , 169.74686 ],\n",
      "        [112.01343 , 165.29564 ],\n",
      "        [107.000496, 156.19975 ],\n",
      "        [121.6489  , 153.56279 ],\n",
      "        [128.1425  , 154.13005 ],\n",
      "        [134.93811 , 153.77411 ],\n",
      "        [150.26378 , 157.37094 ],\n",
      "        [135.12054 , 163.22691 ],\n",
      "        [128.1837  , 163.82823 ],\n",
      "        [121.38966 , 162.90436 ]], dtype=float32), 'roll': 4.591777324676514, 'yaw': 3.403257369995117, 'eye_landmarks': array([[ 93.804146,  96.33963 ],\n",
      "        [ 97.37212 ,  95.5697  ],\n",
      "        [101.66225 ,  92.32498 ],\n",
      "        [106.73232 ,  94.64378 ],\n",
      "        [102.817314,  97.7247  ],\n",
      "        [102.30853 ,  94.067474],\n",
      "        [113.3375  ,  95.52829 ],\n",
      "        [145.44572 ,  96.2257  ],\n",
      "        [151.01941 ,  95.799034],\n",
      "        [155.92686 ,  92.59373 ],\n",
      "        [161.27115 ,  95.34327 ],\n",
      "        [156.68037 ,  98.98886 ],\n",
      "        [155.81761 ,  94.98603 ],\n",
      "        [166.24805 ,  96.76723 ]], dtype=float32), 'fitting_scores_updated': True, 'pitch': 3.16829252243042}                                            ]\n",
      "[{'id': 9942, 'most_recent_fitting_scores': array([1.3001152, 1.3107952, 1.3715945], dtype=float32), 'facial_landmarks': array([[ 67.518   , 100.96661 ],\n",
      "        [ 67.974365, 117.05761 ],\n",
      "        [ 69.48905 , 133.3067  ],\n",
      "        [ 72.304245, 149.34312 ],\n",
      "        [ 77.75321 , 164.9052  ],\n",
      "        [ 86.13749 , 179.28508 ],\n",
      "        [ 97.58931 , 191.49428 ],\n",
      "        [111.79973 , 200.81831 ],\n",
      "        [128.53018 , 203.71169 ],\n",
      "        [145.60414 , 201.01239 ],\n",
      "        [160.22826 , 191.79625 ],\n",
      "        [171.84741 , 179.36621 ],\n",
      "        [180.19557 , 164.78676 ],\n",
      "        [185.23555 , 148.79263 ],\n",
      "        [188.22414 , 132.48026 ],\n",
      "        [189.67754 , 116.1489  ],\n",
      "        [190.05602 , 100.04795 ],\n",
      "        [ 81.85813 ,  82.19386 ],\n",
      "        [ 87.964386,  75.43212 ],\n",
      "        [ 97.04623 ,  72.13601 ],\n",
      "        [107.15948 ,  72.88695 ],\n",
      "        [115.87456 ,  77.13569 ],\n",
      "        [141.23753 ,  77.47304 ],\n",
      "        [149.86209 ,  73.3918  ],\n",
      "        [159.75891 ,  72.293594],\n",
      "        [169.22069 ,  75.41017 ],\n",
      "        [175.60913 ,  82.10747 ],\n",
      "        [128.38571 ,  93.18952 ],\n",
      "        [128.46721 , 103.59579 ],\n",
      "        [128.48267 , 113.71008 ],\n",
      "        [128.5336  , 124.1288  ],\n",
      "        [115.47724 , 132.90793 ],\n",
      "        [121.6833  , 135.70505 ],\n",
      "        [128.53792 , 137.36238 ],\n",
      "        [135.4211  , 135.84094 ],\n",
      "        [141.70174 , 133.28212 ],\n",
      "        [ 92.91473 ,  95.48701 ],\n",
      "        [ 99.01722 ,  91.904785],\n",
      "        [105.68085 ,  91.76606 ],\n",
      "        [112.39056 ,  95.8424  ],\n",
      "        [105.698425,  97.0127  ],\n",
      "        [ 99.075165,  97.07598 ],\n",
      "        [145.6172  ,  95.78435 ],\n",
      "        [151.93634 ,  92.25266 ],\n",
      "        [158.58588 ,  92.53598 ],\n",
      "        [164.66176 ,  96.2469  ],\n",
      "        [158.70432 ,  97.655205],\n",
      "        [152.02988 ,  97.39392 ],\n",
      "        [104.577835, 156.1233  ],\n",
      "        [112.695114, 150.58424 ],\n",
      "        [122.06254 , 148.30186 ],\n",
      "        [128.29794 , 149.85078 ],\n",
      "        [135.13553 , 148.40405 ],\n",
      "        [144.57703 , 151.1292  ],\n",
      "        [152.89832 , 156.64058 ],\n",
      "        [144.81436 , 164.9642  ],\n",
      "        [135.3845  , 169.0522  ],\n",
      "        [128.03705 , 169.64806 ],\n",
      "        [121.31653 , 168.8598  ],\n",
      "        [112.16405 , 164.61737 ],\n",
      "        [107.38427 , 156.04842 ],\n",
      "        [121.90052 , 153.36575 ],\n",
      "        [128.35202 , 153.91412 ],\n",
      "        [135.08998 , 153.58258 ],\n",
      "        [149.9305  , 156.74582 ],\n",
      "        [135.03049 , 162.3295  ],\n",
      "        [128.24849 , 162.94763 ],\n",
      "        [121.645134, 162.11597 ]], dtype=float32), 'roll': 4.281461238861084, 'yaw': 3.5186328887939453, 'eye_landmarks': array([[ 93.49498 ,  95.45207 ],\n",
      "        [ 96.61742 ,  94.83624 ],\n",
      "        [101.154724,  91.49084 ],\n",
      "        [106.290924,  93.88708 ],\n",
      "        [102.264114,  97.27272 ],\n",
      "        [101.9155  ,  93.30114 ],\n",
      "        [113.451004,  94.893906],\n",
      "        [144.34874 ,  96.29541 ],\n",
      "        [150.78836 ,  95.41399 ],\n",
      "        [155.77693 ,  92.15061 ],\n",
      "        [161.02388 ,  94.90974 ],\n",
      "        [156.38217 ,  98.63464 ],\n",
      "        [155.45036 ,  94.678734],\n",
      "        [164.97206 ,  96.513   ]], dtype=float32), 'fitting_scores_updated': True, 'pitch': 2.7711567878723145}                                           ]\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def read_video(filename):\n",
    "    cap = cv2.VideoCapture(filename)\n",
    "    while(cap.isOpened()):\n",
    "        ret, frame = cap.read() # BGR\n",
    "        if ret:\n",
    "            yield frame\n",
    "        else:\n",
    "            break\n",
    "    cap.release()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def linear_interpolate(landmarks, start_idx, stop_idx):\n",
    "    start_landmarks = landmarks[start_idx]\n",
    "    stop_landmarks = landmarks[stop_idx]\n",
    "    delta = stop_landmarks - start_landmarks\n",
    "    for idx in range(1, stop_idx-start_idx):\n",
    "        landmarks[start_idx+idx] = start_landmarks + idx/float(stop_idx-start_idx) * delta\n",
    "    return landmarks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def warp_img(src, dst, img, std_size):\n",
    "    tform = transform.estimate_transform('similarity', src, dst)  # find the transformation matrix\n",
    "    warped = transform.warp(img, inverse_map=tform.inverse, output_shape=std_size)  # wrap the frame image\n",
    "    warped = warped * 255  # note output from wrap is double image (value range [0,1])\n",
    "    warped = warped.astype('uint8')\n",
    "    return warped, tform"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path_for_testing = '/media/taylorpap/1TBM2/DatasetML/lipread_mp4/ABOUT/test/ABOUT_00001.mp4'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "895861e88976e7c9df36d5e7f39807c9551132302d9bca05c7b86b8c01664695"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('projectenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}