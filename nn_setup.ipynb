{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def model_inputs(video_size):\n",
    "\n",
    "    #[batch_size, ?, ?, ?]\n",
    "    #inputs = tf.placeholder(dtype = tf.float32, shape = [None, image_size[0], image_size[1], 3], name= 'videos')\n",
    "    inputs = tf.placeholder(dtype = tf.float32, shape = [batch_size, video_size[0], video_size[1], video_size[2], 1], name= 'videos')\n",
    "    targets = tf.placeholder(dtype= tf.int32, shape = [None], name='targets')\n",
    "    dropout_rate = tf.placeholder(dtype=tf.float32, name='dropout_rate')\n",
    "\n",
    "    return inputs, targets, dropout_rate"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "temp_words_list = ['ABSOLUTELY', 'BUDGET', 'EVERYONE', 'HOUSE', 'MILITARY', 'PUBLIC', 'RESULT', 'SIGNIFICANT', 'WEATHER']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[124 134 180]\n",
      " [124 136 182]\n",
      " [125 137 183]\n",
      " [123 135 183]\n",
      " [120 133 180]\n",
      " [115 131 178]\n",
      " [114 130 177]\n",
      " [115 131 179]\n",
      " [114 130 178]\n",
      " [111 127 175]\n",
      " [107 123 171]\n",
      " [105 119 168]\n",
      " [102 116 165]\n",
      " [100 114 163]\n",
      " [ 99 113 163]\n",
      " [ 97 109 161]\n",
      " [ 96 107 160]\n",
      " [ 97 108 161]\n",
      " [ 97 108 162]\n",
      " [ 99 110 164]\n",
      " [100 111 165]\n",
      " [100 110 165]\n",
      " [100 110 165]\n",
      " [ 98 108 163]\n",
      " [ 96 106 162]\n",
      " [ 94 105 159]\n",
      " [ 97 108 162]\n",
      " [ 98 109 163]\n",
      " [101 111 165]\n",
      " [101 112 164]\n",
      " [100 111 163]\n",
      " [101 108 163]\n",
      " [103 108 165]\n",
      " [ 99 105 158]\n",
      " [ 90  97 148]\n",
      " [ 78  86 134]\n",
      " [ 72  80 126]\n",
      " [ 85  94 139]\n",
      " [107 116 160]\n",
      " [112 121 166]\n",
      " [109 117 164]\n",
      " [111 118 167]\n",
      " [115 122 173]\n",
      " [119 126 176]\n",
      " [119 127 177]\n",
      " [119 126 176]\n",
      " [118 125 175]\n",
      " [117 122 175]\n",
      " [116 118 174]\n",
      " [114 116 171]\n",
      " [110 113 166]\n",
      " [106 109 160]\n",
      " [102 106 154]\n",
      " [ 98 102 149]\n",
      " [ 95 100 145]\n",
      " [ 94 100 144]\n",
      " [ 96 102 144]\n",
      " [ 90  96 138]\n",
      " [ 80  87 129]\n",
      " [ 74  80 123]\n",
      " [ 78  84 127]\n",
      " [ 80  86 129]\n",
      " [ 86  92 136]\n",
      " [ 91  96 141]\n",
      " [ 93  99 145]\n",
      " [ 94 100 146]\n",
      " [ 95 102 148]\n",
      " [ 96 103 149]\n",
      " [ 97 104 150]\n",
      " [ 97 104 150]\n",
      " [ 95 103 151]\n",
      " [ 94 102 150]\n",
      " [ 99 107 156]\n",
      " [100 109 158]\n",
      " [ 99 108 158]\n",
      " [100 109 160]\n",
      " [ 98 107 158]\n",
      " [ 97 106 157]\n",
      " [ 97 107 159]\n",
      " [ 98 109 161]\n",
      " [101 109 161]\n",
      " [103 110 162]\n",
      " [104 111 163]\n",
      " [105 113 164]\n",
      " [106 113 165]\n",
      " [104 112 164]\n",
      " [103 110 162]\n",
      " [101 109 161]\n",
      " [100 107 159]\n",
      " [100 107 159]\n",
      " [100 108 159]\n",
      " [101 109 161]\n",
      " [102 110 162]\n",
      " [103 110 162]\n",
      " [104 111 163]\n",
      " [105 113 164]]\n",
      "(29, 96, 96, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "video_array = np.load('/home/taylorpap/Bootcamp/CroppedLRW/ABOUT/test/ABOUT_00001.npz')['data']\n",
    "print(video_array[0][0])\n",
    "print(video_array.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[143 146 148 147 144 142 139 136 132 128 125 121 121 121 120 117 118 120\n",
      " 121 122 122 122 120 113 112 112 117 117 121 120 119 120 121 119 120 102\n",
      "  88 107 129 130 129 133 139 143 143 139 137 135 133 129 125 121 116 113\n",
      " 113 114 102  91  90  97 104 110 112 111 113 114 115 115 114 113 111 119\n",
      " 122 121 122 118 117 119 123 122 123 124 126 126 125 123 122 122 122 122\n",
      " 122 122 123 124 124 129]\n",
      "(29, 96, 96)\n"
     ]
    }
   ],
   "source": [
    "video_array = np.load('/media/taylorpap/1TBM2/DatasetML/Lipreading_using_Temporal_Convolutional_Networks-master/datasets/visual_data/ABOUT/test/ABOUT_00001.npz')['data']\n",
    "print(video_array[0][0])\n",
    "print(video_array.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0  11  18]\n",
      " [  0  10  17]\n",
      " [  0  16  24]\n",
      " [  5  25  33]\n",
      " [  7  31  42]\n",
      " [  8  33  45]\n",
      " [  6  36  53]\n",
      " [ 13  44  61]\n",
      " [ 26  59  75]\n",
      " [ 39  73  92]\n",
      " [ 48  83 102]\n",
      " [ 51  88 110]\n",
      " [ 55  93 115]\n",
      " [ 59  99 122]\n",
      " [ 61 102 125]\n",
      " [ 60 101 127]\n",
      " [ 56  98 124]\n",
      " [ 53  96 123]\n",
      " [ 52  94 123]\n",
      " [ 50  92 122]\n",
      " [ 55  95 125]\n",
      " [ 60  99 130]\n",
      " [ 68 110 142]\n",
      " [ 81 123 155]\n",
      " [ 90 132 164]\n",
      " [ 91 133 166]\n",
      " [ 88 131 165]\n",
      " [ 86 129 163]\n",
      " [ 85 128 162]\n",
      " [ 86 127 163]\n",
      " [ 87 127 164]\n",
      " [ 89 129 166]\n",
      " [ 98 138 175]\n",
      " [110 148 192]\n",
      " [122 160 206]\n",
      " [126 165 216]\n",
      " [129 169 221]\n",
      " [131 172 225]\n",
      " [133 175 230]\n",
      " [133 176 231]\n",
      " [131 174 229]\n",
      " [128 171 226]\n",
      " [126 169 224]\n",
      " [121 164 218]\n",
      " [116 157 210]\n",
      " [113 153 205]\n",
      " [112 151 202]\n",
      " [112 154 201]\n",
      " [110 154 199]\n",
      " [105 150 193]\n",
      " [103 148 191]\n",
      " [100 145 188]\n",
      " [ 99 144 187]\n",
      " [ 96 141 184]\n",
      " [ 96 141 184]\n",
      " [ 96 141 184]\n",
      " [ 95 140 183]\n",
      " [ 97 142 185]\n",
      " [101 146 189]\n",
      " [104 149 192]\n",
      " [105 150 193]\n",
      " [104 149 192]\n",
      " [104 149 190]\n",
      " [103 149 190]\n",
      " [101 148 189]\n",
      " [ 99 144 184]\n",
      " [ 94 137 176]\n",
      " [ 86 129 168]\n",
      " [ 79 122 161]\n",
      " [ 78 119 158]\n",
      " [ 76 117 156]\n",
      " [ 79 120 159]\n",
      " [ 81 123 162]\n",
      " [ 80 123 162]\n",
      " [ 80 123 162]\n",
      " [ 81 124 163]\n",
      " [ 81 126 165]\n",
      " [ 82 128 166]\n",
      " [ 83 125 170]\n",
      " [ 85 126 172]\n",
      " [ 88 129 175]\n",
      " [ 91 132 178]\n",
      " [ 92 133 179]\n",
      " [ 93 133 179]\n",
      " [ 94 133 179]\n",
      " [ 93 132 178]\n",
      " [ 92 131 177]\n",
      " [ 89 128 174]\n",
      " [ 89 128 174]\n",
      " [ 91 128 174]\n",
      " [ 92 129 175]\n",
      " [ 91 128 174]\n",
      " [ 91 128 174]\n",
      " [ 91 129 173]\n",
      " [ 89 128 172]\n",
      " [ 86 126 170]]\n",
      "(29, 96, 96, 3)\n"
     ]
    }
   ],
   "source": [
    "video_array = np.load('/home/taylorpap/Bootcamp/CroppedLRW/ABSOLUTELY/test/ABSOLUTELY_00006.npz')['data']\n",
    "print(video_array[1][1])\n",
    "print(video_array.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def sparse_accuracy(true_labels, predicted_labels):\n",
    "\n",
    "    assert len(true_labels) == len(predicted_labels)\n",
    "\n",
    "    correct = 0\n",
    "\n",
    "    for i in range(len(true_labels)):\n",
    "\n",
    "        if np.argmax(predicted_labels[i]) == true_labels[i]:\n",
    "            correct +=1\n",
    "\n",
    "    return correct / len(true_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def conv_block(inputs,\n",
    "               number_of_filters,\n",
    "               kernel_size,\n",
    "               strides=(1,1),\n",
    "               padding='SAME',\n",
    "               activation= tf.nn.relu,\n",
    "               max_pool=True,\n",
    "               batch_norm=True):\n",
    "\n",
    "    conv_features = layer = tf.layers.conv2d(inputs=inputs,\n",
    "                                             filters=number_of_filters,\n",
    "                                             kernel_size=kernel_size,\n",
    "                                             strides=strides,\n",
    "                                             padding=padding,\n",
    "                                             activation=activation)\n",
    "\n",
    "    if max_pool:\n",
    "        layer = tf.layers.max_pooling2d(layer,\n",
    "                                        pool_size=(2, 2),\n",
    "                                        strides=(2, 2),\n",
    "                                        padding='SAME')\n",
    "\n",
    "    if batch_norm:\n",
    "        layer = tf.layers.batch_normalization(layer)\n",
    "\n",
    "    return layer, conv_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def conv3d_block(inputs, number_of_filters, kernel_size, strides = None, padding='SAME', activation= tf.nn.relu, max_pool=True):\n",
    "\n",
    "    conv_features = layer = tf.layers.Conv3D(inputs=inputs,\n",
    "                                                filters=number_of_filters,\n",
    "                                                kernel_size=kernel_size,\n",
    "                                                strides=strides,\n",
    "                                                padding=padding,\n",
    "                                                activation=activation)\n",
    "\n",
    "    if max_pool:\n",
    "        layer = tf.layers.MaxPool3D(layer, pool_size = (3,3,3), strides = None, padding = 'SAME')\n",
    "\n",
    "    return layer, conv_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def dense_block(inputs,\n",
    "                units,\n",
    "                activation=tf.nn.relu,\n",
    "                dropout_rate=None,\n",
    "                batch_norm=True):\n",
    "\n",
    "\n",
    "    dense_features = layer = tf.layers.dense(inputs,\n",
    "                                             units=units,\n",
    "                                             activation=activation)\n",
    "\n",
    "    if dropout_rate is not None:\n",
    "        layer = tf.layers.dropout(layer, rate=dropout_rate)\n",
    "\n",
    "    if batch_norm:\n",
    "        layer = tf.layers.batch_normalization(layer)\n",
    "\n",
    "    return layer, dense_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def opt_loss(logits,\n",
    "             targets,\n",
    "             learning_rate):\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=targets, logits=logits))\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "    return loss, optimizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class LipreadVideoModel(object):\n",
    "    def __init__(self, learning_rate, video_dims, number_of_classes):\n",
    "\n",
    "        #tf.reset_default_graph()\n",
    "\n",
    "        self.inputs, self.targets, self.dropout_rate = model_inputs(video_dims)\n",
    "\n",
    "        preprocessed_arrays = self.inputs\n",
    "\n",
    "        #Conv3Dblock 1\n",
    "        conv3d_block_1, self.conv_1_features = conv3d_block(inputs=preprocessed_arrays,\n",
    "                                                            number_of_filters=32,\n",
    "                                                            kernel_size=(3,3,3),\n",
    "                                                            padding='SAME',\n",
    "                                                            activation= tf.nn.relu,\n",
    "                                                            max_pool=False)\n",
    "\n",
    "        #Conv3Dblock 2\n",
    "        conv3d_block_2, self.conv_2_features = conv3d_block(inputs=conv3d_block_1,\n",
    "                                                            number_of_filters=64,\n",
    "                                                            kernel_size=(3,3,3),\n",
    "                                                            padding='SAME',\n",
    "                                                            activation= tf.nn.relu,\n",
    "                                                            max_pool=True)\n",
    "\n",
    "        #Conv3Dblock 3\n",
    "        conv3d_block_3, self.conv_3_features = conv3d_block(inputs=conv3d_block_2,\n",
    "                                                            number_of_filters=64,\n",
    "                                                            kernel_size=(3,3,3),\n",
    "                                                            padding='SAME',\n",
    "                                                            activation= tf.nn.relu,\n",
    "                                                            max_pool=False)\n",
    "\n",
    "        #Conv3Dblock 4\n",
    "        conv3d_block_4, self.conv_4_features = conv3d_block(inputs=conv3d_block_3,\n",
    "                                                            number_of_filters=128,\n",
    "                                                            kernel_size=(5,5,5),\n",
    "                                                            padding='SAME',\n",
    "                                                            activation= tf.nn.relu,\n",
    "                                                            max_pool=True)\n",
    "\n",
    "        #flatten\n",
    "        flat_layer = tf.keras.layers.Flatten(conv3d_block_4)\n",
    "\n",
    "        dense_block_1, dense_1_features = dense_block(flat_layer,\n",
    "                                                      units=512,\n",
    "                                                      activation=tf.nn.relu,\n",
    "                                                      dropout_rate=self.dropout_rate,\n",
    "                                                      batch_norm=True)\n",
    "\n",
    "        dense_block_2, dense_2_features = dense_block(dense_block_1,\n",
    "                                                      units=256,\n",
    "                                                      activation=tf.nn.relu,\n",
    "                                                      dropout_rate=self.dropout_rate,\n",
    "                                                      batch_norm=True)\n",
    "\n",
    "        dense_block_3, dense_3_features = dense_block(dense_block_2,\n",
    "                                                      units=128,\n",
    "                                                      activation=tf.nn.relu,\n",
    "                                                      dropout_rate=self.dropout_rate,\n",
    "                                                      batch_norm=True)\n",
    "\n",
    "        logits = tf.keras.layers.dense(inputs=dense_block_3,\n",
    "                                       units=number_of_classes,\n",
    "                                       activation=None)\n",
    "\n",
    "        self.predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        self.loss, self.opt = opt_loss(logits=logits,\n",
    "                                       targets=self.targets,\n",
    "                                       learning_rate=learning_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'placeholder'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [18]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m first_model \u001B[38;5;241m=\u001B[39m \u001B[43mLipreadVideoModel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.001\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m29\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m88\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m88\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtemp_words_list\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [15]\u001B[0m, in \u001B[0;36mLipreadVideoModel.__init__\u001B[0;34m(self, learning_rate, video_dims, number_of_classes)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, learning_rate, video_dims, number_of_classes):\n\u001B[1;32m      3\u001B[0m \n\u001B[1;32m      4\u001B[0m     \u001B[38;5;66;03m#tf.reset_default_graph()\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minputs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtargets, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout_rate \u001B[38;5;241m=\u001B[39m \u001B[43mmodel_inputs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvideo_dims\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m     preprocessed_arrays \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minputs\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;66;03m#Conv3Dblock 1\u001B[39;00m\n",
      "Input \u001B[0;32mIn [17]\u001B[0m, in \u001B[0;36mmodel_inputs\u001B[0;34m(video_size)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmodel_inputs\u001B[39m(video_size):\n\u001B[1;32m      2\u001B[0m \n\u001B[1;32m      3\u001B[0m     \u001B[38;5;66;03m#[batch_size, ?, ?, ?]\u001B[39;00m\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;66;03m#inputs = tf.placeholder(dtype = tf.float32, shape = [None, image_size[0], image_size[1], 3], name= 'videos')\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mplaceholder\u001B[49m(dtype \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mfloat32, shape \u001B[38;5;241m=\u001B[39m [batch_size, video_size[\u001B[38;5;241m0\u001B[39m], video_size[\u001B[38;5;241m1\u001B[39m], video_size[\u001B[38;5;241m2\u001B[39m], \u001B[38;5;241m1\u001B[39m], name\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvideos\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      6\u001B[0m     targets \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mplaceholder(dtype\u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mint32, shape \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;01mNone\u001B[39;00m], name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtargets\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      7\u001B[0m     dropout_rate \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mplaceholder(dtype\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mfloat32, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdropout_rate\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'tensorflow' has no attribute 'placeholder'"
     ]
    }
   ],
   "source": [
    "first_model = LipreadVideoModel(0.001, (29, 88, 88), len(temp_words_list))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "\n",
    "def train(model,\n",
    "          epochs,\n",
    "          drop_rate,\n",
    "          batch_size,\n",
    "          data,\n",
    "          save_dir,\n",
    "          saver_delta=0.15):\n",
    "\n",
    "    '''\n",
    "    The core training function, use this function to train a model.\n",
    "\n",
    "    :param model: CNN model\n",
    "    :param epochs: integer, number of epochs\n",
    "    :param drop_rate: float, dropout_rate\n",
    "    :param batch_size: integer, number of samples to put through the model at once\n",
    "    :param data: tuple, train-test data Example(X_train, y_train, X_test, y_test)\n",
    "    :param save_dir: string, path to a folder where model checkpoints will be saved\n",
    "    :param saver_delta: float, used to prevent overfitted model to be saved\n",
    "    '''\n",
    "\n",
    "    X_train, y_train, X_test, y_test = data\n",
    "\n",
    "    #start session\n",
    "    session = tf.Session()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    #define saver\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    best_test_accuracy = 0.0\n",
    "    #start training loop\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        train_accuracy = []\n",
    "        train_loss = []\n",
    "\n",
    "        for ii in tqdm(range(len(X_train) // batch_size)):\n",
    "            start_id = ii*batch_size\n",
    "            end_id = start_id + batch_size\n",
    "\n",
    "            X_batch = X_train[start_id:end_id]\n",
    "            y_batch = y_train[start_id:end_id]\n",
    "\n",
    "            feed_dict = {model.inputs:X_batch,\n",
    "                         model.targets:y_batch,\n",
    "                         model.dropout_rate:drop_rate}\n",
    "\n",
    "            _, t_loss, preds_t = session.run([model.optimizer, model.loss, model.predictions], feed_dict=feed_dict)\n",
    "\n",
    "            train_accuracy.append(sparse_accuracy(y_batch, preds_t))\n",
    "            train_loss.append(t_loss)\n",
    "\n",
    "        print(\"Epoch: {}/{}\".format(epoch, epochs),\n",
    "              \" | Training accuracy: {}\".format(np.mean(train_accuracy)),\n",
    "              \" | Training loss: {}\".format(np.mean(train_loss)) )\n",
    "\n",
    "        test_accuracy = []\n",
    "\n",
    "        for ii in tqdm(range(len(X_test) // batch_size)):\n",
    "            start_id = ii*batch_size\n",
    "            end_id = start_id + batch_size\n",
    "\n",
    "            X_batch = X_test[start_id:end_id]\n",
    "            y_batch = y_test[start_id:end_id]\n",
    "\n",
    "            feed_dict = {model.inputs:X_batch,\n",
    "                         model.dropout_rate:0.0}\n",
    "\n",
    "            preds_test = session.run(model.predictions, feed_dict=feed_dict)\n",
    "            test_accuracy.append(sparse_accuracy(y_batch, preds_test))\n",
    "\n",
    "        print(\"Test accuracy: {}\".format(np.mean(test_accuracy)))\n",
    "\n",
    "        #saving the model\n",
    "        if np.mean(train_accuracy) > np.mean(test_accuracy): #to prevent underfitting\n",
    "            if np.abs(np.mean(train_accuracy) - np.mean(test_accuracy)) <= saver_delta: #to prevent overfit\n",
    "                if np.mean(test_accuracy) >= best_test_accuracy:\n",
    "                    best_test_accuracy = np.mean(test_accuracy)\n",
    "                    saver.save(session, \"{}/model_epoch_{}.ckpt\".format(save_dir, epoch))\n",
    "\n",
    "    session.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}