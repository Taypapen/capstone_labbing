{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from fastai.layers import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'Basic Structure for Neural Network:\\n    3D Convolution Blocks -> MaxPool3D -> (Convert to 2D tensor) -> ResNet -> (Flatten) -> 1D Convolution Blocks -> Prediction'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Basic Structure for Neural Network:\n",
    "    3D Convolution Blocks -> MaxPool3D -> (Convert to 2D tensor) -> ResNet -> (Flatten) -> 1D Convolution Blocks -> Prediction'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def noop():\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def threeD_to_2D_tensor(x):\n",
    "    n_batch, n_channels, s_time, sx, sy = x.shape\n",
    "    x = x.transpose(1, 2)\n",
    "    return x.reshape(n_batch*s_time, n_channels, sx, sy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def _3d_block(in_size, out_size, kernel_size, stride, padding, bias=False, relu_type='prelu'):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv3d(in_size, out_size, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "        nn.BatchNorm3d(out_size),\n",
    "        nn.PReLU(num_parameters=out_size) if relu_type== 'prelu' else nn.ReLU()\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def _bottleneck_conv_block(ni, nf, stride):\n",
    "    return nn.Sequential(\n",
    "        ConvLayer(ni,nf//4, 1),\n",
    "        ConvLayer(nf//4,nf//4, stride=stride),\n",
    "        ConvLayer(nf//4, nf, 1, act_cls=None, norm_type=NormType.BatchZero)\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def _conv_block(ni,nf,stride):\n",
    "    return nn.Sequential(\n",
    "        ConvLayer(ni,nf, stride=stride),\n",
    "        ConvLayer(nf, nf, 1, act_cls=None, norm_type=NormType.BatchZero)\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, ni, nf, stride=1, bottled=True):\n",
    "        self.convs = _bottleneck_conv_block(ni, nf, stride) if bottled else _conv_block(ni, nf, stride)\n",
    "        self.idconv = noop if ni==nf else ConvLayer(ni, nf, 1, act_cls=None)\n",
    "        self.pool = noop if stride==1 else nn.AvgPool2d(2, ceil_mode=True)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return F.relu(self.convs(x) + self.idconv(self.pool(x)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class ResNet(nn.Sequential):\n",
    "    def __init__(self, layers, expansion=1):\n",
    "\n",
    "        #self.relu_type= relu_type\n",
    "        self.block_sizes = [64, 64, 128, 256, 512]\n",
    "        for i in range(1, len(self.block_sizes)): self.block_sizes[i] *= expansion\n",
    "        blocks = [self._make_layer(*o) for o in enumerate(layers)]\n",
    "\n",
    "        super().__init__(*blocks, nn.AdaptiveAvgPool2d(1))\n",
    "        #self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        #self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        #self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        #self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        #self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "    def _make_layer(self, idx, n_layers):\n",
    "        stride = 1 if idx==0 else 2\n",
    "        ch_in, ch_out = self.block_sizes[idx:idx+2]\n",
    "        return nn.Sequential(*[\n",
    "            ResBlock(ch_in if i==0 else ch_out, ch_out, stride if i==0 else 1)\n",
    "            for i in range(n_layers)\n",
    "        ])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class BasicBlock1D(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2, relu_type='relu'):\n",
    "        super(BasicBlock1D, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size, stride=stride, padding=padding, dilation=dilation)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(n_outputs)\n",
    "        if relu_type == 'relu':\n",
    "            self.relu1 = nn.ReLU()\n",
    "        elif relu_type == 'prelu':\n",
    "            self.relu1 = nn.PReLU(num_parameters=n_outputs)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.conv2 = nn.Conv1d(n_outputs, n_outputs, kernel_size, stride=stride, padding=padding, dilation=dilation)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(n_outputs)\n",
    "        if relu_type == 'relu':\n",
    "            self.relu2 = nn.ReLU()\n",
    "        elif relu_type == 'prelu':\n",
    "            self.relu2 = nn.PReLU(num_parameters=n_outputs)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        if relu_type == 'relu':\n",
    "            self.relu = nn.ReLU()\n",
    "        elif relu_type == 'prelu':\n",
    "            self.relu = nn.PReLU(num_parameters=n_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.batchnorm1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.dropout1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.batchnorm2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.dropout2(out)\n",
    "\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class ConvNet1D(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, num_classes, kernel_size=3, dropout=0.2, relu_type='relu'):\n",
    "        super(ConvNet1D, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        for i in range(len(num_channels)):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i==0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers.append(BasicBlock1D(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size, padding=1, dropout=dropout, relu_type=relu_type))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.convnet_output = nn.Linear(num_channels[-1], num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.network(x)\n",
    "        return self.convnet_output(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class Lipreading1(nn.Module):\n",
    "    def __init__(self, num_classes, relu_type = 'prelu'):\n",
    "        super(Lipreading1, self).__init__()\n",
    "        self.kernel_size=3\n",
    "        self.dropout=0.2\n",
    "        self.frontend_out = 64\n",
    "        self.backend_out = 512\n",
    "\n",
    "        self.frontend3D = _3d_block(1, self.frontend_out, kernel_size=(3,3,3), stride=(1,2,2), padding=(1,1,1))\n",
    "        self.max_pool1 = nn.MaxPool3d(kernel_size=(1,3,3), stride=(1,2,2), padding=(0,1,1))\n",
    "        self.trunk = ResNet([3,4,6,3], expansion=4)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.tcn = ConvNet1D(self.backend_out, [256, 512], num_classes, kernel_size=self.kernel_size, dropout=self.dropout, relu_type=relu_type)\n",
    "\n",
    "        self._initialize_weights_randomly()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.frontend3D(x)\n",
    "        x = threeD_to_2D_tensor(x)\n",
    "        x = self.trunk(x)\n",
    "        x = self.flatten(x)\n",
    "        return self.tcn(x)\n",
    "\n",
    "    def _initialize_weights_randomly(self):\n",
    "\n",
    "        use_sqrt = True\n",
    "\n",
    "        if use_sqrt:\n",
    "            def f(n):\n",
    "                return math.sqrt( 2.0/float(n) )\n",
    "        else:\n",
    "            def f(n):\n",
    "                return 2.0/float(n)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d) or isinstance(m, nn.Conv2d) or isinstance(m, nn.Conv1d):\n",
    "                n = np.prod( m.kernel_size ) * m.out_channels\n",
    "                m.weight.data.normal_(0, f(n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "\n",
    "            elif isinstance(m, nn.BatchNorm3d) or isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                n = float(m.weight.data[0].nelement())\n",
    "                m.weight.data = m.weight.data.normal_(0, f(n))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}