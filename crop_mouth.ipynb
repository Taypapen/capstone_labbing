{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import cv2\n",
    "import itertools\n",
    "import numpy as np\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "from skimage import transform as transf\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "lips_folder = '/home/taylorpap/Bootcamp/lips_points'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def save2npz(filename, data=None):\n",
    "    if not os.path.exists(os.path.dirname(filename)):\n",
    "        os.makedirs(os.path.dirname(filename))\n",
    "    np.savez_compressed(filename, data=data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#Linear Interpolate on images/frames that are missing landmarks\n",
    "def linear_interpolate(landmarks, start_idx, stop_idx):\n",
    "    start_landmarks = landmarks[start_idx]\n",
    "    stop_landmarks = landmarks[stop_idx]\n",
    "    delta = stop_landmarks - start_landmarks\n",
    "    for idx in range(1, stop_idx-start_idx):\n",
    "        landmarks[start_idx+idx] = start_landmarks + idx/float(stop_idx-start_idx) * delta\n",
    "    return landmarks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def interpolate_missing_landmarks(landmarks):\n",
    "    good_frames = [indexes for indexes, _ in enumerate(landmarks) if _ is not None]\n",
    "    if not good_frames:\n",
    "        return None\n",
    "    for idx in range(1, len(good_frames)):\n",
    "        if good_frames[idx] - good_frames[idx-1] == 1:\n",
    "            continue\n",
    "        else:\n",
    "            landmarks = linear_interpolate(landmarks, good_frames[idx-1], good_frames[idx])\n",
    "    valid_frames_idx = [idx for idx, _ in enumerate(landmarks) if _ is not None]\n",
    "    # -- Corner case: keep frames at the beginning or at the end failed to be detected.\n",
    "    if valid_frames_idx:\n",
    "        landmarks[:valid_frames_idx[0]] = [landmarks[valid_frames_idx[0]]] * valid_frames_idx[0]\n",
    "        landmarks[valid_frames_idx[-1]:] = [landmarks[valid_frames_idx[-1]]] * (len(landmarks) - valid_frames_idx[-1])\n",
    "    return landmarks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "#Create Method for warping image and getting transform parameters\n",
    "def warp_img(src, dst, img, std_size):\n",
    "    tform = transf.estimate_transform('similarity', src, dst)  # find the transformation matrix\n",
    "    warped = transf.warp(img, inverse_map=tform.inverse, output_shape=std_size)  # wrap the frame image\n",
    "    warped = warped * 255  # note output from wrap is double image (value range [0,1])\n",
    "    warped = warped.astype('uint8')\n",
    "    return warped, tform\n",
    "#Create Method to apply a previously calculated transform\n",
    "def apply_transform(transform, img, std_size):\n",
    "    warped = transf.warp(img, inverse_map=transform.inverse, output_shape=std_size)\n",
    "    warped = warped * 255  # note output from wrap is double image (value range [0,1])\n",
    "    warped = warped.astype('uint8')\n",
    "    return warped"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def crop_out_patch(img, landmarks, height, width):\n",
    "    center_x, center_y = np.mean(landmarks, axis=0)\n",
    "\n",
    "    cutted_img = np.copy(img[int(round(center_y) - round(height)): int(round(center_y) + round(height)),\n",
    "                         int(round(center_x) - round(width)): int(round(center_x) + round(width))])\n",
    "    return cutted_img"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "test_none = [{'testnone': np.array([[0.28191882, 0.53480649],\n",
    "        [0.75067544, 0.33600596]])},{'testnone': np.array([None])},{'testnone': np.array([[0.28191882, 0.53480649],\n",
    "        [0.75067544, 0.33600596]])}]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "face_oval_avgs = np.load('/home/taylorpap/Bootcamp/face_oval_averages.npz', allow_pickle=True)['data']\n",
    "face_oval_avgs = face_oval_avgs * 256\n",
    "std_size = (256, 256)\n",
    "landmark_indexes_for_cropping = [2, 3, 10, 11, 26, 30]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def crop_and_return_lip_landmarks(video_path, output_path, oval_landmarks, mouth_landmarks):\n",
    "    vid_capture = cv2.VideoCapture(video_path)\n",
    "    frame_idx = 0\n",
    "    crop_width = 96\n",
    "    crop_height = 96\n",
    "    cropped_size = (crop_width, crop_height)\n",
    "    lips_points = []\n",
    "    if (vid_capture.isOpened() == False):\n",
    "        print(\"Error opening the video file\")\n",
    "    else:\n",
    "        fps = int(vid_capture.get(5))\n",
    "\n",
    "        output_video = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc('m', 'p', '4', 'v'), fps, cropped_size)\n",
    "        while(vid_capture.isOpened()):\n",
    "            ret, frame = vid_capture.read()\n",
    "            if ret:\n",
    "                current_oval = oval_landmarks[frame_idx] *256\n",
    "                current_lips = mouth_landmarks[frame_idx] *256\n",
    "                if frame_idx == 0:\n",
    "                    transformed_frame, trans_mat = warp_img(current_oval[landmark_indexes_for_cropping, :],\n",
    "                                                            face_oval_avgs[landmark_indexes_for_cropping, :],\n",
    "                                                            frame,\n",
    "                                                            std_size)\n",
    "                    sequence = []\n",
    "                trans_lips = trans_mat(current_lips)\n",
    "                trans_frame = apply_transform(trans_mat, frame, std_size)\n",
    "                cut_frame = crop_out_patch(trans_frame, trans_lips, crop_height//2, crop_width//2)\n",
    "                output_video.write(cut_frame)\n",
    "                sequence.append(cut_frame)\n",
    "                lips_points.append(trans_lips)\n",
    "                frame_idx+= 1\n",
    "            else:\n",
    "                break\n",
    "    vid_capture.release()\n",
    "    output_video.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return np.array(lips_points), np.array(sequence)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_none' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [10]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m landmarks \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;01mNone\u001B[39;00m] \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[43mtest_none\u001B[49m)\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m frame_idx \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(landmarks)):\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[0;31mNameError\u001B[0m: name 'test_none' is not defined"
     ]
    }
   ],
   "source": [
    "landmarks = [None] * len(test_none)\n",
    "for frame_idx in range(len(landmarks)):\n",
    "    try:\n",
    "        landmarks[frame_idx] = test_none[frame_idx]['testnone']\n",
    "    except IndexError:\n",
    "        continue\n",
    "test_landmarks = interpolate_missing_landmarks(landmarks)\n",
    "print(test_landmarks)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def interpolate_oval_and_face(landmarks_npz):\n",
    "    landmarks_data = np.load(landmarks_npz, allow_pickle=True)['data']\n",
    "    landmarks_ovals = [None] * len(landmarks_data)\n",
    "    landmarks_lips = [None] * len(landmarks_data)\n",
    "    for frame_idx in range(len(landmarks_ovals)):\n",
    "        try:\n",
    "            landmarks_ovals[frame_idx] = landmarks_data[frame_idx]['oval_landmarks']\n",
    "        except IndexError:\n",
    "            continue\n",
    "    for frame_idx in range(len(landmarks_lips)):\n",
    "        try:\n",
    "            landmarks_lips[frame_idx] = landmarks_data[frame_idx]['lips_landmarks']\n",
    "        except IndexError:\n",
    "            continue\n",
    "    preprocessed_ovals = interpolate_missing_landmarks(landmarks_ovals)\n",
    "    preprocessed_lips = interpolate_missing_landmarks(landmarks_lips)\n",
    "    return preprocessed_ovals, preprocessed_lips"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def interpolate_and_crop(video, landmarks_folder, cropped_save_folder):\n",
    "    assert os.path.isfile(video), \"File does not exist. Path input: {}\".format(video)\n",
    "    dir, file = os.path.split(video)\n",
    "    part, which_folder = os.path.split(dir)\n",
    "    otherpart, word = os.path.split(part)\n",
    "    npz_filename = file[:-4] + \".npz\"\n",
    "\n",
    "    #Get Landmarks path and check that file exists\n",
    "    current_landmarks = os.path.join(landmarks_folder, word, which_folder, npz_filename)\n",
    "    assert os.path.isfile(current_landmarks), \"File does not exist. Path input: {}\".format(current_landmarks)\n",
    "    #Open Landmarks, interpolate missing\n",
    "    oval_landmarks, lips_landmarks = interpolate_oval_and_face(current_landmarks)\n",
    "    if oval_landmarks:\n",
    "        #Create save location for cropped video and transformed lips landmarks\n",
    "        output_save_video = os.path.join(cropped_save_folder, word, which_folder, file)\n",
    "        output_save_video_array_path = os.path.join(cropped_save_folder, word, which_folder, npz_filename)\n",
    "        output_save_lips_points = os.path.join(lips_folder, word, which_folder, npz_filename)\n",
    "\n",
    "        transformed_lips, output_save_video_as_array = crop_and_return_lip_landmarks(video, output_save_video, oval_landmarks, lips_landmarks)\n",
    "        save2npz(output_save_lips_points, data=transformed_lips)\n",
    "        save2npz(output_save_video_array_path, data= output_save_video_as_array)\n",
    "    else:\n",
    "        print(\"No Landmarks in any frame for {}\".format(video))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "lrw_path = '/media/taylorpap/1TBM2/DatasetML/lipread_mp4'\n",
    "landmarks_path = '/home/taylorpap/Bootcamp/LANDMARKS'\n",
    "which_folder = 'test'\n",
    "cropped_save_loc = '/home/taylorpap/Bootcamp/CroppedLRW'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def multiprocess_cropping(lrw_path, landmarks_path, cropped_save_loc, word='*'):\n",
    "    videos = glob.glob(os.path.join(lrw_path, word, which_folder, '*.mp4'))\n",
    "    l= len(videos)\n",
    "    with tqdm(total = l) as pbar:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            futures = [executor.submit(interpolate_and_crop, video, landmarks_path, cropped_save_loc) for video in videos]\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                pbar.update(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 137/25000 [00:05<14:26, 28.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Landmarks in any frame for /media/taylorpap/1TBM2/DatasetML/lipread_mp4/ABUSE/test/ABUSE_00027.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 1555/25000 [00:56<13:15, 29.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Landmarks in any frame for /media/taylorpap/1TBM2/DatasetML/lipread_mp4/ANYTHING/test/ANYTHING_00004.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 6580/25000 [04:06<11:24, 26.92it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Landmarks in any frame for /media/taylorpap/1TBM2/DatasetML/lipread_mp4/DECISION/test/DECISION_00018.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 9086/25000 [05:43<08:43, 30.41it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Landmarks in any frame for /media/taylorpap/1TBM2/DatasetML/lipread_mp4/FORWARD/test/FORWARD_00011.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 11780/25000 [07:23<07:02, 31.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Landmarks in any frame for /media/taylorpap/1TBM2/DatasetML/lipread_mp4/ISSUE/test/ISSUE_00023.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 15238/25000 [09:32<06:12, 26.22it/s]/tmp/ipykernel_55023/1212654754.py:37: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(lips_points), np.array(sequence)\n",
      "100%|██████████| 25000/25000 [15:50<00:00, 26.30it/s]\n"
     ]
    }
   ],
   "source": [
    "#MULTIPROCESS\n",
    "multiprocess_cropping(lrw_path, landmarks_path, cropped_save_loc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "#SINGLE PROCESS\n",
    "\n",
    "#videos = glob.glob(os.path.join(lrw_path, '*', which_folder, '*.mp4'))\n",
    "#for video in tqdm(videos):\n",
    "#    interpolate_and_crop(video, landmarks_path, cropped_save_loc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'oval_landmarks': None, 'lips_landmarks': None}\n",
      " {'oval_landmarks': None, 'lips_landmarks': None}\n",
      " {'oval_landmarks': None, 'lips_landmarks': None}\n",
      " {'oval_landmarks': None, 'lips_landmarks': None}\n",
      " {'oval_landmarks': None, 'lips_landmarks': None}\n",
      " {'oval_landmarks': None, 'lips_landmarks': None}\n",
      " {'oval_landmarks': None, 'lips_landmarks': None}\n",
      " {'oval_landmarks': None, 'lips_landmarks': None}\n",
      " {'oval_landmarks': None, 'lips_landmarks': None}\n",
      " {'oval_landmarks': None, 'lips_landmarks': None}\n",
      " {'oval_landmarks': None, 'lips_landmarks': None}\n",
      " {'oval_landmarks': None, 'lips_landmarks': None}\n",
      " {'oval_landmarks': None, 'lips_landmarks': None}\n",
      " {'oval_landmarks': None, 'lips_landmarks': None}\n",
      " {'oval_landmarks': None, 'lips_landmarks': None}\n",
      " {'oval_landmarks': None, 'lips_landmarks': None}\n",
      " {'oval_landmarks': None, 'lips_landmarks': None}\n",
      " {'oval_landmarks': None, 'lips_landmarks': None}\n",
      " {'oval_landmarks': None, 'lips_landmarks': None}\n",
      " {'oval_landmarks': None, 'lips_landmarks': None}\n",
      " {'oval_landmarks': None, 'lips_landmarks': None}\n",
      " {'oval_landmarks': None, 'lips_landmarks': None}\n",
      " {'oval_landmarks': None, 'lips_landmarks': None}\n",
      " {'oval_landmarks': None, 'lips_landmarks': None}\n",
      " {'oval_landmarks': None, 'lips_landmarks': None}\n",
      " {'oval_landmarks': None, 'lips_landmarks': None}\n",
      " {'oval_landmarks': None, 'lips_landmarks': None}\n",
      " {'oval_landmarks': None, 'lips_landmarks': None}\n",
      " {'oval_landmarks': None, 'lips_landmarks': None}]\n",
      "{'oval_landmarks': None, 'lips_landmarks': None}\n",
      "{'oval_landmarks': None, 'lips_landmarks': None}\n"
     ]
    }
   ],
   "source": [
    "example = np.load('/home/taylorpap/Bootcamp/LANDMARKS/ABUSE/test/ABUSE_00027.npz', allow_pickle=True)['data']\n",
    "print(example)\n",
    "for each_frame in example[2:4]:\n",
    "    print(each_frame)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "interpolate_and_crop('/media/taylorpap/1TBM2/DatasetML/lipread_mp4/ABOUT/test/ABOUT_00001.mp4', landmarks_path, cropped_save_loc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "temp_words_list = ['ABSOLUTELY', 'BUDGET', 'EVERYONE', 'HOUSE', 'MILITARY', 'PUBLIC', 'RESULT', 'SIGNIFICANT', 'WEATHER']\n",
    "which_folder = 'train'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 959/959 [00:35<00:00, 27.17it/s]\n",
      "100%|██████████| 1000/1000 [00:36<00:00, 27.35it/s]\n",
      "100%|██████████| 913/913 [00:33<00:00, 27.13it/s]\n",
      "100%|██████████| 1000/1000 [00:37<00:00, 26.86it/s]\n",
      "100%|██████████| 1000/1000 [00:37<00:00, 26.70it/s]\n",
      "100%|██████████| 1000/1000 [00:37<00:00, 26.90it/s]\n",
      "100%|██████████| 1000/1000 [00:37<00:00, 26.81it/s]\n",
      "100%|██████████| 954/954 [00:36<00:00, 26.18it/s]\n",
      "100%|██████████| 1000/1000 [00:37<00:00, 26.61it/s]\n"
     ]
    }
   ],
   "source": [
    "for testing_words in temp_words_list:\n",
    "   multiprocess_cropping(lrw_path, landmarks_path, cropped_save_loc, word=testing_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "['BUDGET', 'HOUSE', 'MILITARY', 'PUBLIC', 'RESULT']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}