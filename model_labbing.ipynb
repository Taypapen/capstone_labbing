{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "from datasetloadingpy import dataloaders\n",
    "import copy\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "from with_other_resnet import TrueResNet, BasicBlock\n",
    "from super_simple_pytorch_nn import CustomResNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            #for batch_idx, (inputs, lengths, labels) in enumerate(dataloaders[phase]):\n",
    "            for inputs, lengths, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs.unsqueeze(1).cuda(), lengths=lengths)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'test' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def _average_batch(x, lengths):\n",
    "    return torch.stack( [torch.mean( x[index][:,0:i], 1 ) for index, i in enumerate(lengths)],0 )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def threeD_to_2D_tensor(x):\n",
    "    n_batch, n_channels, s_time, sx, sy = x.shape\n",
    "    x = x.transpose(1, 2)\n",
    "    return x.reshape(n_batch*s_time, n_channels, sx, sy)\n",
    "\n",
    "\n",
    "def _3d_block(in_size, out_size, kernel_size, stride, padding, bias=False, relu_type='prelu'):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv3d(in_size, out_size, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "        nn.BatchNorm3d(out_size),\n",
    "        nn.PReLU(num_parameters=out_size) if relu_type== 'prelu' else nn.ReLU()\n",
    "    )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class LipreadLSTMv1(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained=False, relu_type='prelu'):\n",
    "        super(LipreadLSTMv1, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.frontend_out = 64\n",
    "        self.backend_out = 512\n",
    "\n",
    "        self.front3D = _3d_block(1, self.frontend_out, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3), bias=False)\n",
    "        self.max_pool_3D = nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))\n",
    "        self.resnet = TrueResNet(BasicBlock, [2, 2, 2, 2], relu_type=relu_type)\n",
    "        #self.fc = nn.Linear(512 * BasicBlock.expansion, self.backend_out)\n",
    "        #self.bn2 = nn.BatchNorm1d(self.backend_out)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.backend_out,\n",
    "            hidden_size=256,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "        self._initialize_weights_randomly()\n",
    "\n",
    "        if pretrained:\n",
    "            pretrained_model = models.resnet18(pretrained=True)\n",
    "            #pretrained_model.fc = nn.Linear(pretrained_model.fc.in_features, num_classes)\n",
    "            self.resnet.load_state_dict(pretrained_model.state_dict(), strict=False)\n",
    "            for param in self.resnet.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        B, C, T, H, W = x.size()\n",
    "        #print(\"Initial Shape: \" + str(x.shape))\n",
    "        x = self.front3D(x)\n",
    "        #print(\"3D Out Shape: \" + str(x.shape))\n",
    "        x = self.max_pool_3D(x)\n",
    "        #print(\"3D Max Pool Shape: \" + str(x.shape))\n",
    "        Tnew = x.shape[2]\n",
    "        #transposed = x.transpose(1, 2).contiguous()\n",
    "        #x = transposed.view(-1, 64, x.size(3), x.size(4))\n",
    "        x = threeD_to_2D_tensor(x)\n",
    "        #print(\"ResNet In Shape: \" + str(x.shape))\n",
    "        x = self.resnet(x)\n",
    "        #print(\"ResNet Out Shape: \" + str(x.shape))\n",
    "        x = x.view(B, Tnew, x.size(1))\n",
    "        #x = x.transpose(1, 2)\n",
    "        #print(\"DataTransform Out Shape: \" + str(x.shape))\n",
    "\n",
    "        #x = self.fc(x)\n",
    "        #x = self.bn2(x)\n",
    "        #print(\"First Batch Norm OutShape: \" + str(x.shape))\n",
    "        #x = x.view(x.shape[0], -1, self.num_classes)\n",
    "        x, _ = self.lstm(x)\n",
    "        #print(\"LSTM Out Shape: \" + str(x.shape))\n",
    "        x = x.transpose(1, 2)\n",
    "        x = _average_batch(x, lengths)\n",
    "        #print(\"Avg Batch OutShape: \" + str(x.shape))\n",
    "        x = self.fc2(x)\n",
    "        #print(\"Last Liner Out Shape: \" + str(x.shape))\n",
    "        return x\n",
    "\n",
    "\n",
    "    def _initialize_weights_randomly(self):\n",
    "\n",
    "        use_sqrt = True\n",
    "\n",
    "        if use_sqrt:\n",
    "            def f(n):\n",
    "                return math.sqrt( 2.0/float(n) )\n",
    "        else:\n",
    "            def f(n):\n",
    "                return 2.0/float(n)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d) or isinstance(m, nn.Conv2d) or isinstance(m, nn.Conv1d):\n",
    "                n = np.prod( m.kernel_size ) * m.out_channels\n",
    "                m.weight.data.normal_(0, f(n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "\n",
    "            elif isinstance(m, nn.BatchNorm3d) or isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                n = float(m.weight.data[0].nelement())\n",
    "                m.weight.data = m.weight.data.normal_(0, f(n))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition train loaded\n",
      "Partition test loaded\n"
     ]
    }
   ],
   "source": [
    "data_path = '/home/taylorpap/Bootcamp/CroppedLRW'\n",
    "temp_words_list = ['ABSOLUTELY', 'BUDGET', 'EVERYONE', 'HOUSE', 'MILITARY', 'PUBLIC', 'RESULT', 'SIGNIFICANT',\n",
    "                   'WEATHER']\n",
    "new_temp_words_list = ['BUDGET']\n",
    "datasets = dataloaders(data_dir=data_path, label_fp=temp_words_list, batch_size=32, workers=8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([32, 29, 88, 88])\n",
      "Labels batch shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "train_features, train_lengths, train_labels = next(iter(datasets['train']))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "model_ft = LipreadLSTMv1(len(temp_words_list), pretrained=True)\n",
    "\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "LipreadLSTMv1(\n  (front3D): Sequential(\n    (0): Conv3d(1, 64, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3), bias=False)\n    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): PReLU(num_parameters=64)\n  )\n  (max_pool_3D): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), dilation=1, ceil_mode=False)\n  (resnet): TrueResNet(\n    (layer1): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): PReLU(num_parameters=64)\n        (relu2): PReLU(num_parameters=64)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): PReLU(num_parameters=64)\n        (relu2): PReLU(num_parameters=64)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (layer2): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): PReLU(num_parameters=128)\n        (relu2): PReLU(num_parameters=128)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): PReLU(num_parameters=128)\n        (relu2): PReLU(num_parameters=128)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (layer3): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): PReLU(num_parameters=256)\n        (relu2): PReLU(num_parameters=256)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): PReLU(num_parameters=256)\n        (relu2): PReLU(num_parameters=256)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (layer4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): PReLU(num_parameters=512)\n        (relu2): PReLU(num_parameters=512)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): PReLU(num_parameters=512)\n        (relu2): PReLU(num_parameters=512)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (avgpool): AdaptiveAvgPool2d(output_size=1)\n  )\n  (lstm): LSTM(512, 256, num_layers=2, batch_first=True, bidirectional=True)\n  (fc2): Linear(in_features=512, out_features=9, bias=True)\n)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 2.1761 Acc: 0.1556\n",
      "test Loss: 2.1539 Acc: 0.1711\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 2.1383 Acc: 0.1798\n",
      "test Loss: 2.1315 Acc: 0.1689\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 2.1065 Acc: 0.1973\n",
      "test Loss: 2.0901 Acc: 0.2089\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 2.0861 Acc: 0.2139\n",
      "test Loss: 2.0898 Acc: 0.2044\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 2.0662 Acc: 0.2193\n",
      "test Loss: 2.1184 Acc: 0.2156\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 2.0517 Acc: 0.2357\n",
      "test Loss: 2.0722 Acc: 0.2000\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 2.0235 Acc: 0.2508\n",
      "test Loss: 2.0993 Acc: 0.2067\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 2.0037 Acc: 0.2644\n",
      "test Loss: 2.0217 Acc: 0.2378\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 1.9895 Acc: 0.2684\n",
      "test Loss: 2.0112 Acc: 0.2489\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 1.9852 Acc: 0.2688\n",
      "test Loss: 2.0110 Acc: 0.2511\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 1.9808 Acc: 0.2702\n",
      "test Loss: 2.0084 Acc: 0.2600\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 1.9730 Acc: 0.2752\n",
      "test Loss: 2.0074 Acc: 0.2422\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 1.9725 Acc: 0.2747\n",
      "test Loss: 2.0024 Acc: 0.2467\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 1.9666 Acc: 0.2881\n",
      "test Loss: 1.9970 Acc: 0.2489\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 1.9683 Acc: 0.2782\n",
      "test Loss: 1.9959 Acc: 0.2467\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 1.9624 Acc: 0.2828\n",
      "test Loss: 1.9947 Acc: 0.2556\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 1.9613 Acc: 0.2870\n",
      "test Loss: 1.9930 Acc: 0.2511\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 1.9594 Acc: 0.2873\n",
      "test Loss: 1.9931 Acc: 0.2556\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 1.9581 Acc: 0.2862\n",
      "test Loss: 1.9937 Acc: 0.2622\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 1.9612 Acc: 0.2887\n",
      "test Loss: 1.9941 Acc: 0.2511\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 1.9565 Acc: 0.2856\n",
      "test Loss: 1.9920 Acc: 0.2511\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 1.9610 Acc: 0.2875\n",
      "test Loss: 1.9907 Acc: 0.2600\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 1.9574 Acc: 0.2837\n",
      "test Loss: 1.9914 Acc: 0.2600\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 1.9601 Acc: 0.2856\n",
      "test Loss: 1.9914 Acc: 0.2556\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 1.9540 Acc: 0.2914\n",
      "test Loss: 1.9944 Acc: 0.2533\n",
      "\n",
      "Training complete in 12m 54s\n",
      "Best val Acc: 0.262222\n"
     ]
    }
   ],
   "source": [
    "model_ft = train_model(model_ft, datasets, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=25)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}